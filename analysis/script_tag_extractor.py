#!/usr/bin/env python3
"""
Ù…Ø³ØªØ®Ø±Ø¬ script tags - Script Tag Extractor
ÙŠØ³ØªØ®Ø±Ø¬ Ø¬Ù…ÙŠØ¹ script tags ÙˆÙŠØ­Ù„Ù„ ØªÙƒÙˆÙŠÙ†Ø§ØªÙ‡Ø§ Ø¨Ø¯Ù‚Ø©
"""

import os
import re
import json
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from collections import defaultdict

class ScriptTagExtractor:
    def __init__(self, static_dir='.'):
        self.static_dir = static_dir
        self.script_data = {
            'external_scripts': [],
            'inline_scripts': [],
            'config_objects': [],
            'api_endpoints': [],
            'third_party_services': defaultdict(list)
        }
    
    def extract_from_html(self, filepath):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ scripts Ù…Ù† HTML"""
        try:
            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ external scripts
                for script in soup.find_all('script', src=True):
                    self._process_external_script(script, filepath)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ inline scripts
                for script in soup.find_all('script', src=False):
                    self._process_inline_script(script, filepath)
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ config objects Ù…Ù† ØµÙØ­Ø©
                self._extract_config_objects(soup, filepath)
        
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {filepath}: {e}")
    
    def _process_external_script(self, script, filepath):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© external script tag"""
        src = script.get('src', '')
        if not src:
            return
        
        # ØªØ­Ù„ÙŠÙ„ URL
        parsed = urlparse(src)
        domain = parsed.netloc or 'local'
        
        script_info = {
            'file': os.path.basename(filepath),
            'src': src,
            'domain': domain,
            'path': parsed.path,
            'async': script.has_attr('async'),
            'defer': script.has_attr('defer'),
            'type': script.get('type', 'text/javascript'),
            'integrity': script.get('integrity', ''),
            'crossorigin': script.get('crossorigin', '')
        }
        
        self.script_data['external_scripts'].append(script_info)
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø®Ø¯Ù…Ø©
        service = self._identify_service(domain, src)
        if service:
            self.script_data['third_party_services'][service].append(script_info)
    
    def _process_inline_script(self, script, filepath):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© inline script"""
        content = script.string or ''
        if not content.strip():
            return
        
        script_info = {
            'file': os.path.basename(filepath),
            'length': len(content),
            'type': script.get('type', 'text/javascript'),
            'has_config': self._contains_configuration(content),
            'snippet': content[:500].strip()
        }
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ API calls
        api_calls = self._extract_api_calls(content)
        if api_calls:
            script_info['api_calls'] = api_calls
            self.script_data['api_endpoints'].extend(api_calls)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ configurations
        configs = self._extract_configurations(content)
        if configs:
            script_info['configurations'] = configs
        
        self.script_data['inline_scripts'].append(script_info)
    
    def _extract_config_objects(self, soup, filepath):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ configuration objects"""
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† __NEXT_DATA__
        next_data_scripts = soup.find_all('script', id='__NEXT_DATA__')
        for script in next_data_scripts:
            try:
                data = json.loads(script.string or '{}')
                self.script_data['config_objects'].append({
                    'file': os.path.basename(filepath),
                    'type': 'Next.js Data',
                    'has_props': 'props' in data,
                    'has_apollo_state': 'apolloState' in str(data),
                    'build_id': data.get('buildId', ''),
                    'page': data.get('page', '')
                })
            except json.JSONDecodeError:
                pass
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† dataLayer (GTM)
        for script in soup.find_all('script'):
            content = script.string or ''
            if 'dataLayer' in content:
                self.script_data['config_objects'].append({
                    'file': os.path.basename(filepath),
                    'type': 'Google Tag Manager DataLayer',
                    'snippet': self._extract_datalayer_config(content)
                })
    
    def _extract_datalayer_config(self, content):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙƒÙˆÙŠÙ† dataLayer"""
        match = re.search(r'dataLayer\s*=\s*(\[.*?\])', content, re.DOTALL)
        if match:
            return match.group(1)[:500]
        return ''
    
    def _contains_configuration(self, content):
        """ÙØ­Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙƒÙˆÙŠÙ†Ø§Øª"""
        config_patterns = [
            r'config\s*=\s*{',
            r'settings\s*=\s*{',
            r'options\s*=\s*{',
            r'init\(',
            r'configure\(',
            r'apiKey\s*:',
            r'projectId\s*:'
        ]
        
        for pattern in config_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                return True
        return False
    
    def _extract_api_calls(self, content):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ API calls Ù…Ù† Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª"""
        api_calls = []
        
        # patterns Ù„Ù€ API calls
        patterns = [
            r'fetch\([\'"]([^\'"]+)[\'"]',
            r'axios\.[a-z]+\([\'"]([^\'"]+)[\'"]',
            r'\.get\([\'"]([^\'"]+)[\'"]',
            r'\.post\([\'"]([^\'"]+)[\'"]',
            r'XMLHttpRequest.*?open\([\'"][A-Z]+[\'"],\s*[\'"]([^\'"]+)[\'"]'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                url = match.group(1)
                if url.startswith('http') or url.startswith('//'):
                    api_calls.append(url)
        
        return list(set(api_calls))
    
    def _extract_configurations(self, content):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙƒÙˆÙŠÙ†Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª"""
        configs = []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ API keys (Ù…Ø®ÙÙŠØ©)
        api_key_patterns = [
            r'apiKey\s*:\s*[\'"]([^\'"]+)[\'"]',
            r'api_key\s*=\s*[\'"]([^\'"]+)[\'"]',
            r'key\s*:\s*[\'"]([^\'"]+)[\'"]'
        ]
        
        for pattern in api_key_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                key = match.group(1)
                # Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù…ÙØªØ§Ø­
                configs.append({
                    'type': 'API Key',
                    'value': key[:10] + '...' if len(key) > 10 else key
                })
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ project IDs
        project_patterns = [
            r'projectId\s*:\s*[\'"]([^\'"]+)[\'"]',
            r'project_id\s*=\s*[\'"]([^\'"]+)[\'"]'
        ]
        
        for pattern in project_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                configs.append({
                    'type': 'Project ID',
                    'value': match.group(1)
                })
        
        return configs
    
    def _identify_service(self, domain, src):
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø¯Ù…Ø© Ù…Ù† domain"""
        services = {
            'googletagmanager.com': 'Google Tag Manager',
            'google-analytics.com': 'Google Analytics',
            'amplitude.com': 'Amplitude',
            'segment.com': 'Segment',
            'cdn.segment.com': 'Segment',
            'stripe.com': 'Stripe',
            'js.stripe.com': 'Stripe',
            'firebase.google.com': 'Firebase',
            'firebaseapp.com': 'Firebase',
            'cdn.sanity.io': 'Sanity CMS',
            'launchdarkly.com': 'LaunchDarkly',
            'statsig.com': 'Statsig',
            'datadoghq.com': 'Datadog',
            'coframe.ai': 'Coframe',
            'hotjar.com': 'Hotjar',
            'intercom.io': 'Intercom',
            'sentry.io': 'Sentry',
            'mixpanel.com': 'Mixpanel',
            'facebook.net': 'Facebook Pixel',
            'connect.facebook.net': 'Facebook Pixel',
            'appsflyer.com': 'AppsFlyer',
            'recaptcha': 'reCAPTCHA',
            'gstatic.com': 'Google Static Assets'
        }
        
        for key, service in services.items():
            if key in domain or key in src:
                return service
        
        return None
    
    def analyze_all_files(self):
        """ØªØ­Ù„ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª HTML"""
        print("ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ script tags Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª...")
        
        html_files = list(Path(self.static_dir).glob('**/*.html'))
        print(f"ğŸ“„ Ù…Ø¹Ø§Ù„Ø¬Ø© {len(html_files)} Ù…Ù„Ù HTML...")
        
        for html_file in html_files:
            self.extract_from_html(html_file)
    
    def generate_report(self):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„"""
        report = {
            'summary': {
                'total_external_scripts': len(self.script_data['external_scripts']),
                'total_inline_scripts': len(self.script_data['inline_scripts']),
                'total_config_objects': len(self.script_data['config_objects']),
                'total_api_endpoints': len(self.script_data['api_endpoints']),
                'total_services': len(self.script_data['third_party_services'])
            },
            'external_scripts': self.script_data['external_scripts'],
            'inline_scripts': self.script_data['inline_scripts'],
            'config_objects': self.script_data['config_objects'],
            'api_endpoints': list(set(self.script_data['api_endpoints'])),
            'third_party_services': dict(self.script_data['third_party_services'])
        }
        
        return report
    
    def save_report(self, output_file='analysis/script_tags_report.json'):
        """Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
        report = self.generate_report()
        
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± script tags ÙÙŠ: {output_file}")
        print(f"ğŸ“Š External Scripts: {report['summary']['total_external_scripts']}")
        print(f"ğŸ“œ Inline Scripts: {report['summary']['total_inline_scripts']}")
        print(f"âš™ï¸ Config Objects: {report['summary']['total_config_objects']}")
        print(f"ğŸŒ API Endpoints: {report['summary']['total_api_endpoints']}")
        print(f"ğŸ”Œ Third Party Services: {report['summary']['total_services']}")
        
        return report

def main():
    extractor = ScriptTagExtractor()
    extractor.analyze_all_files()
    report = extractor.save_report()
    
    print("\n" + "="*60)
    print("ğŸ”Œ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
    print("="*60)
    for service, scripts in report['third_party_services'].items():
        print(f"âœ“ {service}: {len(scripts)} script(s)")

if __name__ == '__main__':
    main()
